import streamlit as st
import requests
import os
from dotenv import load_dotenv
from groq import Groq
import json
from datetime import datetime
from typing import List, Dict

# Carregar variÃ¡veis do .env
load_dotenv()

# ConfiguraÃ§Ã£o das APIs
serpapi_key = os.getenv("SERPAPI_KEY")
groq_api_key = os.getenv("GROQ_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Inicializar clientes
if groq_api_key:
    groq_client = Groq(api_key=groq_api_key)
else:
    groq_client = None

if openai_api_key:
    from openai import OpenAI
    openai_client = OpenAI(api_key=openai_api_key)
else:
    openai_client = None

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="ğŸ” Motor de Busca Inteligente",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado para melhorar a aparÃªncia
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .search-result {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        background: white;
    }
    .source-link {
        background: #f0f2f6;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
    }
    .groq-badge {
        background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
        color: white;
        padding: 0.3rem 0.8rem;
        border-radius: 20px;
        font-size: 0.8rem;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# Interface principal
st.markdown("""
<div class="main-header">
    <h1>ğŸ” Motor de Busca Inteligente</h1>
    <p>Busque qualquer tema e receba um resumo detalhado com IA</p>
    <span class="groq-badge">ğŸ†• OpenAI OSS 120B + GroqCloud</span>
</div>
""", unsafe_allow_html=True)

# Sidebar com configuraÃ§Ãµes
with st.sidebar:
    st.header("âš™ï¸ ConfiguraÃ§Ãµes")
    
    # BotÃµes de controle
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ Limpar Cache", help="Limpa todos os dados em cache"):
            st.cache_data.clear()
            st.cache_resource.clear()
            st.success("Cache limpo!")
    
    with col2:
        if st.button("ğŸ”„ Resetar App", help="Reseta completamente a aplicaÃ§Ã£o"):
            st.cache_data.clear()
            st.cache_resource.clear()
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.success("App resetado!")
            st.rerun()
    
    # Verificar status das APIs
    st.subheader("Status das APIs")
    if serpapi_key:
        st.success("âœ… SerpAPI configurada")
    else:
        st.error("âŒ SerpAPI nÃ£o configurada")
    
    if groq_api_key:
        st.success("âœ… GroqCloud configurada")
    else:
        st.error("âŒ GroqCloud nÃ£o configurada")
        
    if openai_api_key:
        st.success("âœ… OpenAI configurada")
    else:
        st.error("âŒ OpenAI nÃ£o configurada")
    
    # ConfiguraÃ§Ãµes de busca
    st.subheader("ConfiguraÃ§Ãµes de Busca")
    num_results = st.slider("NÃºmero de resultados", 3, 20, 10)
    language = st.selectbox("Idioma", ["pt", "en", "es"], index=0)
    country = st.selectbox("PaÃ­s", ["br", "us", "es", "global"], index=0)
    
    # ConfiguraÃ§Ãµes do modelo
    st.subheader("ConfiguraÃ§Ãµes da IA")
    
    # Seletor de provedor
    ai_provider = st.selectbox(
        "Provedor de IA",
        ["GroqCloud", "OpenAI"],
        index=0,
        help="Escolha entre GroqCloud (rÃ¡pido) ou OpenAI (modelos o1)"
    )
    
    if ai_provider == "OpenAI":
        model_choice = st.selectbox(
            "Modelo OpenAI", 
            [
                "o1",                           # Novo modelo o1 (120B equivalente)
                "o1-preview",                   # o1-preview (reasoning)
                "o1-mini",                      # o1-mini (mais rÃ¡pido)
                "gpt-4o",                       # GPT-4o mais recente
                "gpt-4o-mini",                  # GPT-4o mini
                "gpt-3.5-turbo"                 # ClÃ¡ssico
            ],
            index=0,
            help="Modelos OpenAI - o1 Ã© o mais avanÃ§ado (reasoning)"
        )
        
        # InformaÃ§Ãµes sobre modelos OpenAI
        openai_info = {
            "o1": "ğŸ§  OpenAI o1 - Reasoning avanÃ§ado, modelo mais inteligente",
            "o1-preview": "ğŸ”¬ o1-preview - VersÃ£o de desenvolvimento do o1", 
            "o1-mini": "âš¡ o1-mini - Reasoning rÃ¡pido e eficiente",
            "gpt-4o": "ğŸš€ GPT-4o - Multimodal e versÃ¡til",
            "gpt-4o-mini": "ğŸ’¨ GPT-4o mini - RÃ¡pido e econÃ´mico",
            "gpt-3.5-turbo": "ğŸ“ GPT-3.5 - ClÃ¡ssico e confiÃ¡vel"
        }
        st.info(openai_info.get(model_choice, "Modelo selecionado"))
        
    else:  # GroqCloud
        model_choice = st.selectbox(
            "Modelo GroqCloud", 
            [
                "openai/gpt-oss-120b",         # ğŸ†• OpenAI OSS 120B (flagship)
                "openai/gpt-oss-20b",          # ğŸ†• OpenAI OSS 20B (eficiente)
                "llama-3.3-70b-versatile",      # Llama 3.3 
                "llama3-70b-8192",              # Llama 3 estÃ¡vel
                "llama3-8b-8192",               # Mais rÃ¡pido
                "mixtral-8x7b-32768",           # Contexto longo
                "gemma2-9b-it",                 # Eficiente
            ],
            index=0,
            help="Modelos GroqCloud - Agora com OpenAI OSS!"
        )
        
        # InformaÃ§Ã£o sobre o modelo GroqCloud selecionado
        groq_info = {
            "openai/gpt-oss-120b": "ğŸš€ OpenAI OSS 120B - Flagship open-source da OpenAI (NOVO!)",
            "openai/gpt-oss-20b": "âš¡ OpenAI OSS 20B - Eficiente e rÃ¡pido (NOVO!)",
            "llama-3.3-70b-versatile": "ğŸ¦™ Llama 3.3 - Meta's mais recente",
            "llama3-70b-8192": "ğŸ’ª Llama 3 70B - Balanceado e confiÃ¡vel", 
            "llama3-8b-8192": "âš¡ Llama 3 8B - RÃ¡pido e eficiente",
            "mixtral-8x7b-32768": "ğŸ“š Mixtral - Melhor para textos longos",
            "gemma2-9b-it": "ğŸ¯ Gemma2 - Otimizado e preciso"
        }
        st.info(groq_info.get(model_choice, "Modelo selecionado"))
    
    temperature = st.slider("Criatividade (Temperature)", 0.0, 1.0, 0.2)
    max_tokens = st.slider("Tamanho do resumo", 300, 2000, 800)
    
    # Nota especial para modelos especiais
    if ai_provider == "OpenAI" and model_choice.startswith("o1"):
        st.warning("âš ï¸ Modelos o1 usam reasoning interno e podem demorar mais, mas sÃ£o mais precisos!")
    elif ai_provider == "GroqCloud" and "gpt-oss" in model_choice:
        st.success("ğŸ†• Modelo OpenAI OSS - Open Source da OpenAI rodando no GroqCloud!")
    
    # InformaÃ§Ãµes sobre velocidade
    if ai_provider == "GroqCloud":
        if "gpt-oss-120b" in model_choice:
            st.info("ğŸš€ OpenAI OSS 120B: ~500+ tokens/s - Qualidade OpenAI com velocidade Groq!")
        elif "gpt-oss-20b" in model_choice:
            st.info("âš¡ OpenAI OSS 20B: ~1000+ tokens/s - Ultra rÃ¡pido!")
        else:
            st.info("âš¡ GroqCloud oferece inferÃªncia ultrarrÃ¡pida!")
    else:
        st.info("ğŸ§  OpenAI o1 oferece reasoning avanÃ§ado para anÃ¡lises profundas!")

def search_web(query: str, num_results: int = 10) -> List[Dict]:
    """Buscar informaÃ§Ãµes na web usando SerpAPI"""
    if not serpapi_key:
        st.error("âŒ SERPAPI_KEY nÃ£o configurada. Adicione no arquivo .env")
        return []
    
    params = {
        "q": query,
        "api_key": serpapi_key,
        "engine": "google",
        "num": num_results,
        "hl": language,
        "gl": country if country != "global" else "us",
        "safe": "active",
        "tbm": "nws" if st.session_state.get('search_news', False) else None
    }
    
    try:
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        results = response.json()
        
        # Extrair diferentes tipos de resultados
        all_results = []
        
        # Resultados orgÃ¢nicos
        organic_results = results.get("organic_results", [])
        for item in organic_results:
            all_results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "source": item.get("displayed_link", ""),
                "type": "web"
            })
        
        # Resultados de notÃ­cias
        news_results = results.get("news_results", [])
        for item in news_results:
            all_results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "source": item.get("source", ""),
                "type": "news",
                "date": item.get("date", "")
            })
        
        # Knowledge graph (se disponÃ­vel)
        knowledge_graph = results.get("knowledge_graph", {})
        if knowledge_graph:
            all_results.insert(0, {
                "title": knowledge_graph.get("title", ""),
                "link": knowledge_graph.get("website", ""),
                "snippet": knowledge_graph.get("description", ""),
                "source": "Knowledge Graph",
                "type": "knowledge"
            })
        
        return all_results
        
    except requests.RequestException as e:
        st.error(f"Erro na requisiÃ§Ã£o: {e}")
        return []
    except Exception as e:
        st.error(f"Erro inesperado: {e}")
        return []

def generate_summary_with_ai(query: str, search_results: List[Dict], ai_provider: str, model_choice: str) -> str:
    """Gerar resumo usando OpenAI ou GroqCloud"""
    
    # Preparar contexto dos resultados
    context = ""
    for i, result in enumerate(search_results[:12], 1):  # Limitar a 12 resultados
        context += f"[FONTE {i}]\n"
        context += f"TÃ­tulo: {result['title']}\n"
        context += f"Tipo: {result['type'].upper()}\n"
        context += f"Fonte: {result['source']}\n"
        if result.get('date'):
            context += f"Data: {result['date']}\n"
        context += f"ConteÃºdo: {result['snippet']}\n"
        context += f"Link: {result['link']}\n\n"
    
    # Prompt otimizado para anÃ¡lise
    if model_choice.startswith("o1"):
        # Prompt especial para modelos o1 (reasoning)
        prompt = f"""Analise profundamente as informaÃ§Ãµes sobre "{query}" e forneÃ§a uma anÃ¡lise estruturada e detalhada.

FONTES DISPONÃVEIS:
{context}

Como um analista especializado, crie um resumo abrangente que demonstre raciocÃ­nio crÃ­tico e anÃ¡lise profunda. Use apenas as informaÃ§Ãµes das fontes fornecidas.

Estruture sua resposta com:
- Resumo executivo dos pontos principais
- AnÃ¡lise detalhada com insights crÃ­ticos
- TendÃªncias e padrÃµes identificados
- ImplicaÃ§Ãµes e perspectivas futuras
- ConclusÃµes fundamentadas

Seja preciso, analÃ­tico e use markdown para formataÃ§Ã£o."""
    else:
        # Prompt padrÃ£o para outros modelos
        prompt = f"""VocÃª Ã© um analista especializado em sÃ­ntese de informaÃ§Ãµes. Analise as fontes sobre "{query}" e crie um resumo completo e estruturado.

FONTES CONSULTADAS:
{context}

INSTRUÃ‡Ã•ES:
1. Crie um resumo abrangente e bem fundamentado
2. Use APENAS informaÃ§Ãµes das fontes fornecidas
3. Organize o conteÃºdo de forma lÃ³gica e fluida
4. Destaque tendÃªncias e padrÃµes importantes
5. Mantenha neutralidade e objetividade
6. Use formataÃ§Ã£o markdown para clareza
7. Cite insights de diferentes fontes quando relevante

ESTRUTURA OBRIGATÃ“RIA:
## ğŸ¯ Resumo Executivo
[SÃ­ntese dos pontos principais em 2-3 parÃ¡grafos]

## ğŸ“Š AnÃ¡lise Detalhada
[Desenvolvimento aprofundado dos temas centrais]

## ğŸ” Insights Principais
â€¢ [Ponto relevante 1]
â€¢ [Ponto relevante 2] 
â€¢ [Ponto relevante 3]
â€¢ [Outros insights importantes]

## ğŸ“ˆ TendÃªncias e Perspectivas
[AnÃ¡lise de tendÃªncias e projeÃ§Ãµes quando aplicÃ¡vel]

## ğŸ’¡ ConclusÃµes
[SÃ­ntese final e consideraÃ§Ãµes importantes]

Responda APENAS com o resumo estruturado. Seja preciso e informativo."""
    
    try:
        if ai_provider == "OpenAI":
            if not openai_client:
                return "âŒ OpenAI nÃ£o configurada. Configure OPENAI_API_KEY no arquivo .env"
            
            # ConfiguraÃ§Ãµes especÃ­ficas para modelos o1
            if model_choice.startswith("o1"):
                response = openai_client.chat.completions.create(
                    model=model_choice,
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    # Modelos o1 nÃ£o suportam system message, temperature, etc.
                )
            else:
                response = openai_client.chat.completions.create(
                    model=model_choice,
                    messages=[
                        {
                            "role": "system", 
                            "content": "VocÃª Ã© um especialista em anÃ¡lise de informaÃ§Ãµes que cria resumos precisos e bem estruturados baseados em fontes web confiÃ¡veis."
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
            
            return response.choices[0].message.content
            
        else:  # GroqCloud
            if not groq_client:
                return "âŒ GroqCloud nÃ£o configurada. Configure GROQ_API_KEY no arquivo .env"
            
            response = groq_client.chat.completions.create(
                model=model_choice,
                messages=[
                    {
                        "role": "system", 
                        "content": "VocÃª Ã© um especialista em anÃ¡lise de informaÃ§Ãµes que cria resumos precisos e bem estruturados baseados em fontes web confiÃ¡veis."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                stream=False
            )
            
            return response.choices[0].message.content
        
    except Exception as e:
        return f"âŒ Erro ao gerar resumo: {str(e)}\n\nVerifique se sua chave de API estÃ¡ correta."

def display_sources(search_results: List[Dict]):
    """Exibir fontes de forma organizada"""
    st.subheader("ğŸ“š Fontes Consultadas")
    
    # Separar por tipo
    knowledge_results = [r for r in search_results if r.get('type') == 'knowledge']
    news_results = [r for r in search_results if r.get('type') == 'news']
    web_results = [r for r in search_results if r.get('type') == 'web']
    
    # Exibir em abas
    tab1, tab2, tab3 = st.tabs(["ğŸŒ Web", "ğŸ“° NotÃ­cias", "ğŸ§  Knowledge Graph"])
    
    with tab1:
        if web_results:
            for i, result in enumerate(web_results, 1):
                with st.expander(f"{i}. {result['title'][:70]}..."):
                    st.write(f"**ğŸ”— Fonte:** {result['source']}")
                    st.write(f"**ğŸ“ Resumo:** {result['snippet']}")
                    st.markdown(f"[â¡ï¸ Acessar link completo]({result['link']})")
        else:
            st.info("Nenhum resultado web encontrado.")
    
    with tab2:
        if news_results:
            for i, result in enumerate(news_results, 1):
                with st.expander(f"{i}. {result['title'][:70]}..."):
                    st.write(f"**ğŸ“° Fonte:** {result['source']}")
                    if result.get('date'):
                        st.write(f"**ğŸ“… Data:** {result['date']}")
                    st.write(f"**ğŸ“ Resumo:** {result['snippet']}")
                    st.markdown(f"[â¡ï¸ Ler notÃ­cia completa]({result['link']})")
        else:
            st.info("Nenhuma notÃ­cia encontrada.")
    
    with tab3:
        if knowledge_results:
            for result in knowledge_results:
                st.write(f"**ğŸ§  TÃ­tulo:** {result['title']}")
                st.write(f"**ğŸ“ DescriÃ§Ã£o:** {result['snippet']}")
                if result['link']:
                    st.markdown(f"[â¡ï¸ Mais informaÃ§Ãµes]({result['link']})")
        else:
            st.info("Nenhum Knowledge Graph disponÃ­vel.")

# Interface principal
col1, col2, col3 = st.columns([3, 1, 1])

with col1:
    query = st.text_input(
        "ğŸ” Digite sua pesquisa:",
        placeholder="Ex: inteligÃªncia artificial 2024, economia brasileira, tecnologia...",
        help="Digite qualquer tema para buscar informaÃ§Ãµes atualizadas e receber anÃ¡lise por IA"
    )

with col2:
    search_type = st.selectbox("Tipo", ["Geral", "NotÃ­cias"], key="search_type")
    if search_type == "NotÃ­cias":
        st.session_state['search_news'] = True
    else:
        st.session_state['search_news'] = False

with col3:
    search_button = st.button("ğŸš€ Buscar", type="primary", use_container_width=True)

# Executar busca
if query and (search_button or st.session_state.get('last_query') != query):
    st.session_state['last_query'] = query
    
    if not serpapi_key:
        st.error("âš ï¸ Configure SERPAPI_KEY no arquivo .env")
    elif ai_provider == "OpenAI" and not openai_api_key:
        st.error("âš ï¸ Configure OPENAI_API_KEY no arquivo .env para usar modelos OpenAI")
    elif ai_provider == "GroqCloud" and not groq_api_key:
        st.error("âš ï¸ Configure GROQ_API_KEY no arquivo .env para usar GroqCloud")
    else:
        # Container para resultados
        results_container = st.container()
        
        with results_container:
            # Mostrar progresso
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Passo 1: Buscar na web
            status_text.text("ğŸ” Buscando informaÃ§Ãµes na web...")
            progress_bar.progress(30)
            
            search_results = search_web(query, num_results)
            
            if not search_results:
                progress_bar.empty()
                status_text.empty()
                st.warning("âŒ Nenhum resultado encontrado. Tente termos diferentes ou verifique sua conexÃ£o.")
            else:
                # Passo 2: Gerar resumo com IA
                if ai_provider == "GroqCloud" and "gpt-oss" in model_choice:
                    provider_text = "ğŸš€ Analisando com OpenAI OSS..."
                elif model_choice.startswith("o1"):
                    provider_text = "ğŸ§  o1 Reasoning..."
                else:
                    provider_text = f"âš¡ Analisando com {ai_provider}..."
                    
                status_text.text(provider_text)
                progress_bar.progress(70)
                
                # Cronometrar a geraÃ§Ã£o
                start_time = datetime.now()
                summary = generate_summary_with_ai(query, search_results, ai_provider, model_choice)
                end_time = datetime.now()
                generation_time = (end_time - start_time).total_seconds()
                
                # Passo 3: Finalizar
                status_text.text("âœ… AnÃ¡lise concluÃ­da!")
                progress_bar.progress(100)
                
                # Limpar indicadores
                progress_bar.empty()
                status_text.empty()
                
                # MÃ©tricas detalhadas
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ğŸ“Š Fontes", len(search_results))
                with col2:
                    web_count = len([r for r in search_results if r.get('type') == 'web'])
                    st.metric("ğŸŒ Sites", web_count)
                with col3:
                    news_count = len([r for r in search_results if r.get('type') == 'news'])
                    st.metric("ğŸ“° NotÃ­cias", news_count)
                with col4:
                    st.metric("âš¡ Velocidade", f"{generation_time:.1f}s")
                
                # Resumo principal
                st.markdown("---")
                st.markdown("## ğŸ“‹ AnÃ¡lise Inteligente")
                
                # Exibir o resumo
                if "âŒ" not in summary:
                    st.markdown(summary)
                else:
                    st.error(summary)
                
                # Fontes
                st.markdown("---")
                display_sources(search_results)
                
                # InformaÃ§Ãµes adicionais
                st.markdown("---")
                info_col1, info_col2 = st.columns(2)
                with info_col1:
                    st.info(f"ğŸ•’ Pesquisa realizada: {datetime.now().strftime('%d/%m/%Y Ã s %H:%M')}")
                with info_col2:
                    if ai_provider == "OpenAI":
                        if model_choice.startswith("o1"):
                            st.success(f"ğŸ§  Reasoning completo em {generation_time:.1f}s")
                        else:
                            st.success(f"ğŸ¤– OpenAI processou em {generation_time:.1f}s")
                    else:
                        if "gpt-oss" in model_choice:
                            st.success(f"ğŸš€ OpenAI OSS processou em {generation_time:.1f}s")
                        else:
                            st.success(f"âš¡ GroqCloud processou em {generation_time:.1f}s")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; margin-top: 2rem;'>
    <p>ğŸ’¡ <strong>Motor de Busca Inteligente</strong></p>
    <p>ğŸš€ Powered by <strong>OpenAI OSS 120B</strong> + <strong>GroqCloud</strong> + SerpAPI</p>
    <p><small>Configure GROQ_API_KEY e SERPAPI_KEY no arquivo .env</small></p>
</div>
""", unsafe_allow_html=True)

# Dicas na sidebar
with st.sidebar:
    st.markdown("---")
    st.subheader("ğŸ’¡ Dicas de Uso")
    st.markdown("""
    **ğŸ†• OpenAI OSS vs Outros:**
    - ğŸš€ **OSS 120B**: Qualidade OpenAI + Velocidade Groq
    - âš¡ **OSS 20B**: Ultra rÃ¡pido (1000+ t/s)
    - ğŸ¦™ **Llama**: Open source tradicional
    
    **Termos especÃ­ficos** funcionam melhor:
    - âœ… "IA generativa trends 2025"
    - âœ… "Bitcoin anÃ¡lise mercado"
    - âŒ "tecnologia" (muito amplo)
    
    **Para notÃ­cias** use o filtro "NotÃ­cias"
    """)
    
    st.markdown("---")
    st.markdown("**ğŸ”‘ APIs NecessÃ¡rias:**")
    st.markdown("- [GroqCloud](https://console.groq.com) (Gratuito + OSS)")
    st.markdown("- [SerpAPI](https://serpapi.com) (Busca web)")
    
    st.markdown("---")
    st.subheader("ğŸ†• OpenAI OSS Models")
    st.markdown("""
    **GPT-OSS 120B**: Mixture-of-Experts (MoE) com 20B parÃ¢metros ativos e 128 experts
    
    **Performance**: Near-parity com o4-mini em reasoning, roda em single 80GB GPU
    
    **Velocidade**: 500+ tokens/s (120B) e 1000+ tokens/s (20B)
    """)
