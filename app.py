import streamlit as st
import requests
import os
from dotenv import load_dotenv
from groq import Groq
import json
from datetime import datetime
from typing import List, Dict

# Carregar vari√°veis do .env
load_dotenv()

# Configura√ß√£o das APIs
serpapi_key = os.getenv("SERPAPI_KEY")
groq_api_key = os.getenv("GROQ_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Inicializar clientes
if groq_api_key:
    groq_client = Groq(api_key=groq_api_key)
else:
    groq_client = None

if openai_api_key:
    from openai import OpenAI
    openai_client = OpenAI(api_key=openai_api_key)
else:
    openai_client = None

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="üîç Motor de Busca Inteligente",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado para melhorar a apar√™ncia
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .search-result {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        background: white;
    }
    .source-link {
        background: #f0f2f6;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
    }
    .groq-badge {
        background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
        color: white;
        padding: 0.3rem 0.8rem;
        border-radius: 20px;
        font-size: 0.8rem;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# Interface principal
st.markdown("""
<div class="main-header">
    <h1>üîç Motor de Busca Inteligente</h1>
    <p>Busque qualquer tema e receba um resumo detalhado com IA</p>
    <span class="groq-badge">üÜï OpenAI OSS 120B + GroqCloud</span>
</div>
""", unsafe_allow_html=True)

# Sidebar com configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # Bot√µes de controle
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üóëÔ∏è Limpar Cache", help="Limpa todos os dados em cache"):
            st.cache_data.clear()
            st.cache_resource.clear()
            st.success("Cache limpo!")
    
    with col2:
        if st.button("üîÑ Resetar App", help="Reseta completamente a aplica√ß√£o"):
            st.cache_data.clear()
            st.cache_resource.clear()
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.success("App resetado!")
            st.rerun()
    
    # Verificar status das APIs
    st.subheader("Status das APIs")
    if serpapi_key:
        st.success("‚úÖ SerpAPI configurada")
    else:
        st.error("‚ùå SerpAPI n√£o configurada")
    
    if groq_api_key:
        st.success("‚úÖ GroqCloud configurada")
    else:
        st.error("‚ùå GroqCloud n√£o configurada")
        
    if openai_api_key:
        st.success("‚úÖ OpenAI configurada")
    else:
        st.error("‚ùå OpenAI n√£o configurada")
    
    # Configura√ß√µes de busca
    st.subheader("Configura√ß√µes de Busca")
    num_results = st.slider("N√∫mero de resultados", 3, 20, 10)
    language = st.selectbox("Idioma", ["pt", "en", "es"], index=0)
    country = st.selectbox("Pa√≠s", ["br", "us", "es", "global"], index=0)
    
    # Configura√ß√µes do modelo
    st.subheader("Configura√ß√µes da IA")
    
    # Seletor de provedor
    ai_provider = st.selectbox(
        "Provedor de IA",
        ["GroqCloud", "OpenAI"],
        index=0,
        help="Escolha entre GroqCloud (r√°pido) ou OpenAI (modelos o1)"
    )
    
    if ai_provider == "OpenAI":
        model_choice = st.selectbox(
            "Modelo OpenAI", 
            [
                "o1",                           # Novo modelo o1 (120B equivalente)
                "o1-preview",                   # o1-preview (reasoning)
                "o1-mini",                      # o1-mini (mais r√°pido)
                "gpt-4o",                       # GPT-4o mais recente
                "gpt-4o-mini",                  # GPT-4o mini
                "gpt-3.5-turbo"                 # Cl√°ssico
            ],
            index=0,
            help="Modelos OpenAI - o1 √© o mais avan√ßado (reasoning)"
        )
        
        # Informa√ß√µes sobre modelos OpenAI
        openai_info = {
            "o1": "üß† OpenAI o1 - Reasoning avan√ßado, modelo mais inteligente",
            "o1-preview": "üî¨ o1-preview - Vers√£o de desenvolvimento do o1", 
            "o1-mini": "‚ö° o1-mini - Reasoning r√°pido e eficiente",
            "gpt-4o": "üöÄ GPT-4o - Multimodal e vers√°til",
            "gpt-4o-mini": "üí® GPT-4o mini - R√°pido e econ√¥mico",
            "gpt-3.5-turbo": "üìù GPT-3.5 - Cl√°ssico e confi√°vel"
        }
        st.info(openai_info.get(model_choice, "Modelo selecionado"))
        
    else:  # GroqCloud
        model_choice = st.selectbox(
            "Modelo GroqCloud", 
            [
                "openai/gpt-oss-120b",         # üÜï OpenAI OSS 120B (flagship)
                "openai/gpt-oss-20b",          # üÜï OpenAI OSS 20B (eficiente)
                "llama-3.3-70b-versatile",      # Llama 3.3 
                "llama3-70b-8192",              # Llama 3 est√°vel
                "llama3-8b-8192",               # Mais r√°pido
                "mixtral-8x7b-32768",           # Contexto longo
                "gemma2-9b-it",                 # Eficiente
            ],
            index=0,
            help="Modelos GroqCloud - Agora com OpenAI OSS!"
        )
        
        # Informa√ß√£o sobre o modelo GroqCloud selecionado
        groq_info = {
            "openai/gpt-oss-120b": "üöÄ OpenAI OSS 120B - Flagship open-source da OpenAI (NOVO!)",
            "openai/gpt-oss-20b": "‚ö° OpenAI OSS 20B - Eficiente e r√°pido (NOVO!)",
            "llama-3.3-70b-versatile": "ü¶ô Llama 3.3 - Meta's mais recente",
            "llama3-70b-8192": "üí™ Llama 3 70B - Balanceado e confi√°vel", 
            "llama3-8b-8192": "‚ö° Llama 3 8B - R√°pido e eficiente",
            "mixtral-8x7b-32768": "üìö Mixtral - Melhor para textos longos",
            "gemma2-9b-it": "üéØ Gemma2 - Otimizado e preciso"
        }
        st.info(groq_info.get(model_choice, "Modelo selecionado"))
    
    temperature = st.slider("Criatividade (Temperature)", 0.0, 1.0, 0.2)
    max_tokens = st.slider("Tamanho do resumo", 300, 2000, 800)
    
    # Nota especial para modelos especiais
    if ai_provider == "OpenAI" and model_choice.startswith("o1"):
        st.warning("‚ö†Ô∏è Modelos o1 usam reasoning interno e podem demorar mais, mas s√£o mais precisos!")
    elif ai_provider == "GroqCloud" and "gpt-oss" in model_choice:
        st.success("üÜï Modelo OpenAI OSS - Open Source da OpenAI rodando no GroqCloud!")
    
    # Informa√ß√µes sobre velocidade
    if ai_provider == "GroqCloud":
        if "gpt-oss-120b" in model_choice:
            st.info("üöÄ OpenAI OSS 120B: ~500+ tokens/s - Qualidade OpenAI com velocidade Groq!")
        elif "gpt-oss-20b" in model_choice:
            st.info("‚ö° OpenAI OSS 20B: ~1000+ tokens/s - Ultra r√°pido!")
        else:
            st.info("‚ö° GroqCloud oferece infer√™ncia ultrarr√°pida!")
    else:
        st.info("üß† OpenAI o1 oferece reasoning avan√ßado para an√°lises profundas!")

def search_web(query: str, num_results: int = 10) -> List[Dict]:
    """Buscar informa√ß√µes na web usando SerpAPI"""
    if not serpapi_key:
        st.error("‚ùå SERPAPI_KEY n√£o configurada. Adicione no arquivo .env")
        return []
    
    params = {
        "q": query,
        "api_key": serpapi_key,
        "engine": "google",
        "num": num_results,
        "hl": language,
        "gl": country if country != "global" else "us",
        "safe": "active",
        "tbm": "nws" if st.session_state.get('search_news', False) else None
    }
    
    try:
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        results = response.json()
        
        # Extrair diferentes tipos de resultados
        all_results = []
        
        # Resultados org√¢nicos
        organic_results = results.get("organic_results", [])
        for item in organic_results:
            all_results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "source": item.get("displayed_link", ""),
                "type": "web"
            })
        
        # Resultados de not√≠cias
        news_results = results.get("news_results", [])
        for item in news_results:
            all_results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "source": item.get("source", ""),
                "type": "news",
                "date": item.get("date", "")
            })
        
        # Knowledge graph (se dispon√≠vel)
        knowledge_graph = results.get("knowledge_graph", {})
        if knowledge_graph:
            all_results.insert(0, {
                "title": knowledge_graph.get("title", ""),
                "link": knowledge_graph.get("website", ""),
                "snippet": knowledge_graph.get("description", ""),
                "source": "Knowledge Graph",
                "type": "knowledge"
            })
        
        return all_results
        
    except requests.RequestException as e:
        st.error(f"Erro na requisi√ß√£o: {e}")
        return []
    except Exception as e:
        st.error(f"Erro inesperado: {e}")
        return []

def generate_summary_with_ai(query: str, search_results: List[Dict], ai_provider: str, model_choice: str) -> str:
    """Gerar resumo usando OpenAI ou GroqCloud"""
    
    # Preparar contexto dos resultados
    context = ""
    for i, result in enumerate(search_results[:12], 1):  # Limitar a 12 resultados
        context += f"[FONTE {i}]\n"
        context += f"T√≠tulo: {result['title']}\n"
        context += f"Tipo: {result['type'].upper()}\n"
        context += f"Fonte: {result['source']}\n"
        if result.get('date'):
            context += f"Data: {result['date']}\n"
        context += f"Conte√∫do: {result['snippet']}\n"
        context += f"Link: {result['link']}\n\n"
    
    # Prompt otimizado para an√°lise
    if model_choice.startswith("o1"):
        # Prompt especial para modelos o1 (reasoning)
        prompt = f"""Analise profundamente as informa√ß√µes sobre "{query}" e forne√ßa uma an√°lise estruturada e detalhada.

FONTES DISPON√çVEIS:
{context}

Como um analista especializado, crie um resumo abrangente que demonstre racioc√≠nio cr√≠tico e an√°lise profunda. Use apenas as informa√ß√µes das fontes fornecidas.

Estruture sua resposta com:
- Resumo executivo dos pontos principais
- An√°lise detalhada com insights cr√≠ticos
- Tend√™ncias e padr√µes identificados
- Implica√ß√µes e perspectivas futuras
- Conclus√µes fundamentadas

Seja preciso, anal√≠tico e use markdown para formata√ß√£o."""
    else:
        # Prompt padr√£o para outros modelos
        prompt = f"""Voc√™ √© um analista especializado em s√≠ntese de informa√ß√µes. Analise as fontes sobre "{query}" e crie um resumo completo e estruturado.

FONTES CONSULTADAS:
{context}

INSTRU√á√ïES:
1. Crie um resumo abrangente e bem fundamentado
2. Use APENAS informa√ß√µes das fontes fornecidas
3. Organize o conte√∫do de forma l√≥gica e fluida
4. Destaque tend√™ncias e padr√µes importantes
5. Mantenha neutralidade e objetividade
6. Use formata√ß√£o markdown para clareza
7. Cite insights de diferentes fontes quando relevante

ESTRUTURA OBRIGAT√ìRIA:
## üéØ Resumo Executivo
[S√≠ntese dos pontos principais em 2-3 par√°grafos]

## üìä An√°lise Detalhada
[Desenvolvimento aprofundado dos temas centrais]

## üîç Insights Principais
‚Ä¢ [Ponto relevante 1]
‚Ä¢ [Ponto relevante 2] 
‚Ä¢ [Ponto relevante 3]
‚Ä¢ [Outros insights importantes]

## üìà Tend√™ncias e Perspectivas
[An√°lise de tend√™ncias e proje√ß√µes quando aplic√°vel]

## üí° Conclus√µes
[S√≠ntese final e considera√ß√µes importantes]

Responda APENAS com o resumo estruturado. Seja preciso e informativo."""
    
    try:
        if ai_provider == "OpenAI":
            if not openai_client:
                return "‚ùå OpenAI n√£o configurada. Configure OPENAI_API_KEY no arquivo .env"
            
            # Configura√ß√µes espec√≠ficas para modelos o1
            if model_choice.startswith("o1"):
                response = openai_client.chat.completions.create(
                    model=model_choice,
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    # Modelos o1 n√£o suportam system message, temperature, etc.
                )
            else:
                response = openai_client.chat.completions.create(
                    model=model_choice,
                    messages=[
                        {
                            "role": "system", 
                            "content": "Voc√™ √© um especialista em an√°lise de informa√ß√µes que cria resumos precisos e bem estruturados baseados em fontes web confi√°veis."
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
            
            return response.choices[0].message.content
            
        else:  # GroqCloud
            if not groq_client:
                return "‚ùå GroqCloud n√£o configurada. Configure GROQ_API_KEY no arquivo .env"
            
            response = groq_client.chat.completions.create(
                model=model_choice,
                messages=[
                    {
                        "role": "system", 
                        "content": "Voc√™ √© um especialista em an√°lise de informa√ß√µes que cria resumos precisos e bem estruturados baseados em fontes web confi√°veis."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                stream=False
            )
            
            return response.choices[0].message.content
        
    except Exception as e:
        return f"‚ùå Erro ao gerar resumo: {str(e)}\n\nVerifique se sua chave de API est√° correta."

def display_sources(search_results: List[Dict]):
    """Exibir fontes de forma organizada"""
    st.subheader("üìö Fontes Consultadas")
    
    # Separar por tipo
    knowledge_results = [r for r in search_results if r.get('type') == 'knowledge']
    news_results = [r for r in search_results if r.get('type') == 'news']
    web_results = [r for r in search_results if r.get('type') == 'web']
    
    # Exibir em abas
    tab1, tab2, tab3 = st.tabs(["üåê Web", "üì∞ Not√≠cias", "üß† Knowledge Graph"])
    
    with tab1:
        if web_results:
            for i, result in enumerate(web_results, 1):
                with st.expander(f"{i}. {result['title'][:70]}..."):
                    st.write(f"**üîó Fonte:** {result['source']}")
                    st.write(f"**üìù Resumo:** {result['snippet']}")
                    st.markdown(f"[‚û°Ô∏è Acessar link completo]({result['link']})")
        else:
            st.info("Nenhum resultado web encontrado.")
    
    with tab2:
        if news_results:
            for i, result in enumerate(news_results, 1):
                with st.expander(f"{i}. {result['title'][:70]}..."):
                    st.write(f"**üì∞ Fonte:** {result['source']}")
                    if result.get('date'):
                        st.write(f"**üìÖ Data:** {result['date']}")
                    st.write(f"**üìù Resumo:** {result['snippet']}")
                    st.markdown(f"[‚û°Ô∏è Ler not√≠cia completa]({result['link']})")
        else:
            st.info("Nenhuma not√≠cia encontrada.")
    
    with tab3:
        if knowledge_results:
            for result in knowledge_results:
                st.write(f"**üß† T√≠tulo:** {result['title']}")
                st.write(f"**üìù Descri√ß√£o:** {result['snippet']}")
                if result['link']:
                    st.markdown(f"[‚û°Ô∏è Mais informa√ß√µes]({result['link']})")
        else:
            st.info("Nenhum Knowledge Graph dispon√≠vel.")

# Interface principal
col1, col2, col3 = st.columns([3, 1, 1])

with col1:
    query = st.text_input(
        "üîç Digite sua pesquisa:",
        placeholder="Ex: intelig√™ncia artificial 2024, economia brasileira, tecnologia...",
        help="Digite qualquer tema para buscar informa√ß√µes atualizadas e receber an√°lise por IA"
    )

with col2:
    search_type = st.selectbox("Tipo", ["Geral", "Not√≠cias"], key="search_type")
    if search_type == "Not√≠cias":
        st.session_state['search_news'] = True
    else:
        st.session_state['search_news'] = False

with col3:
    search_button = st.button("üöÄ Buscar", type="primary", use_container_width=True)

# Executar busca
if query and (search_button or st.session_state.get('last_query') != query):
    st.session_state['last_query'] = query
    
    if not serpapi_key:
        st.error("‚ö†Ô∏è Configure SERPAPI_KEY no arquivo .env")
    elif ai_provider == "OpenAI" and not openai_api_key:
        st.error("‚ö†Ô∏è Configure OPENAI_API_KEY no arquivo .env para usar modelos OpenAI")
    elif ai_provider == "GroqCloud" and not groq_api_key:
        st.error("‚ö†Ô∏è Configure GROQ_API_KEY no arquivo .env para usar GroqCloud")
    else:
        # Container para resultados
        results_container = st.container()
        
        with results_container:
            # Mostrar progresso
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Passo 1: Buscar na web
            status_text.text("üîç Buscando informa√ß√µes na web...")
            progress_bar.progress(30)
            
            search_results = search_web(query, num_results)
            
            if not search_results:
                progress_bar.empty()
                status_text.empty()
                st.warning("‚ùå Nenhum resultado encontrado. Tente termos diferentes ou verifique sua conex√£o.")
            else:
                # Passo 2: Gerar resumo com IA
                if ai_provider == "GroqCloud" and "gpt-oss" in model_choice:
                    provider_text = "üöÄ Analisando com OpenAI OSS..."
                elif model_choice.startswith("o1"):
                    provider_text = "üß† o1 Reasoning..."
                else:
                    provider_text = f"‚ö° Analisando com {ai_provider}..."
                    
                status_text.text(provider_text)
                progress_bar.progress(70)
                
                # Cronometrar a gera√ß√£o
                start_time = datetime.now()
                summary = generate_summary_with_ai(query, search_results, ai_provider, model_choice)
                end_time = datetime.now()
                generation_time = (end_time - start_time).total_seconds()
                
                # Passo 3: Finalizar
                status_text.text("‚úÖ An√°lise conclu√≠da!")
                progress_bar.progress(100)
                
                # Limpar indicadores
                progress_bar.empty()
                status_text.empty()
                
                # M√©tricas detalhadas
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üìä Fontes", len(search_results))
                with col2:
                    web_count = len([r for r in search_results if r.get('type') == 'web'])
                    st.metric("üåê Sites", web_count)
                with col3:
                    news_count = len([r for r in search_results if r.get('type') == 'news'])
                    st.metric("üì∞ Not√≠cias", news_count)
                with col4:
                    st.metric("‚ö° Velocidade", f"{generation_time:.1f}s")
                
                # Resumo principal
                st.markdown("---")
                st.markdown("## üìã An√°lise Inteligente")
                
                # Exibir o resumo
                if "‚ùå" not in summary:
                    st.markdown(summary)
                else:
                    st.error(summary)
                
                # Fontes
                st.markdown("---")
                display_sources(search_results)
                
                # Informa√ß√µes adicionais
                st.markdown("---")
                info_col1, info_col2 = st.columns(2)
                with info_col1:
                    st.info(f"üïí Pesquisa realizada: {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}")
                with info_col2:
                    if ai_provider == "OpenAI":
                        if model_choice.startswith("o1"):
                            st.success(f"üß† Reasoning completo em {generation_time:.1f}s")
                        else:
                            st.success(f"ü§ñ OpenAI processou em {generation_time:.1f}s")
                    else:
                        if "gpt-oss" in model_choice:
                            st.success(f"üöÄ OpenAI OSS processou em {generation_time:.1f}s")
                        else:
                            st.success(f"‚ö° GroqCloud processou em {generation_time:.1f}s")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; margin-top: 2rem;'>
    <p>üí° <strong>Motor de Busca Inteligente</strong></p>
    <p>üöÄ Powered by <strong>OpenAI OSS 120B</strong> + <strong>GroqCloud</strong> + SerpAPI</p>
    <p><small>Configure GROQ_API_KEY e SERPAPI_KEY no arquivo .env</small></p>
</div>
""", unsafe_allow_html=True)

# Dicas na sidebar
with st.sidebar:
    st.markdown("---")
    st.subheader("üí° Dicas de Uso")
    st.markdown("""
    **üÜï OpenAI OSS vs Outros:**
    - üöÄ **OSS 120B**: Qualidade OpenAI + Velocidade Groq
    - ‚ö° **OSS 20B**: Ultra r√°pido (1000+ t/s)
    - ü¶ô **Llama**: Open source tradicional
    
    **Termos espec√≠ficos** funcionam melhor:
    - ‚úÖ "IA generativa trends 2025"
    - ‚úÖ "Bitcoin an√°lise mercado"
    - ‚ùå "tecnologia" (muito amplo)
    
    **Para not√≠cias** use o filtro "Not√≠cias"
    """)
    
    st.markdown("---")
    st.markdown("**üîë APIs Necess√°rias:**")
    st.markdown("- [GroqCloud](https://console.groq.com) (Gratuito + OSS)")
    st.markdown("- [SerpAPI](https://serpapi.com) (Busca web)")
    
    st.markdown("---")
    st.subheader("üÜï OpenAI OSS Models")
    st.markdown("""
    **GPT-OSS 120B**: Mixture-of-Experts (MoE) com 20B par√¢metros ativos e 128 experts
    
    **Performance**: Near-parity com o4-mini em reasoning, roda em single 80GB GPU
    
    **Velocidade**: 500+ tokens/s (120B) e 1000+ tokens/s (20B)
    """)
